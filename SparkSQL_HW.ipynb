{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chatrawit/main/blob/main/SparkSQL_HW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmXtCWIesz9y"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# **SparkSQL HOMEWORK**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN2kud2usz90"
      },
      "source": [
        "***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDg63D7hsz90"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFPBIeYcsz91"
      },
      "outputs": [],
      "source": [
        "# Installing required packages\n",
        "!pip install pyspark\n",
        "!pip install findspark\n",
        "!pip install pyarrow==1.0.0\n",
        "!pip install pandas\n",
        "!pip install numpy==1.19.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uISn0e6bsz91"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7P82hT_sz92"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRHZr2IIsz92"
      },
      "source": [
        "## Exercise 1 -  Spark session\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AngU3NlUsz92"
      },
      "source": [
        "Create and initialize the Spark session needed to load the data frames and operate on it\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN4CO3dxsz92"
      },
      "source": [
        "#### Task : Creating the spark session and context\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here"
      ],
      "metadata": {
        "id": "odPL9-Uot-8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O26Q-Ponsz93"
      },
      "source": [
        "## Exercise 2 - Loading the Data and creating a table view\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: Set Directory \n",
        "\n",
        "\n",
        "Set google colab folder directory"
      ],
      "metadata": {
        "id": "9Cy0wA2ruLrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "path=(\"your directory\")"
      ],
      "metadata": {
        "id": "jxt-pnyst_PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kV8f8pssz93"
      },
      "source": [
        "#### Task 2: Load data into a Pandas DataFrame.\n",
        "\n",
        "Pandas has a convenient function to load CSV data from a URL directly into a pandas dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaMsYpO5sz93"
      },
      "outputs": [],
      "source": [
        "# Read the file using `read_csv` function in pandas\n",
        "iris = pd.read_csv(path+'iris.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview a few records\n",
        "iris.head()"
      ],
      "metadata": {
        "id": "K-VBPxYguvJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBLPlcQ8sz94"
      },
      "source": [
        "#### Task 3: Loading data into a Spark DataFrame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWq5Y0ousz94"
      },
      "source": [
        "We use the `createDataFrame` function to load the data into a spark dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntRA743Rsz94"
      },
      "outputs": [],
      "source": [
        "sdf = spark.createDataFrame(iris) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnAOhCkksz94"
      },
      "source": [
        "Let us look at the schema of the loaded spark dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJ2Uh_Zcsz94",
        "outputId": "585c7bb4-38c3-443a-9ce4-a816a20cecf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8968baa5d855>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sdf' is not defined"
          ]
        }
      ],
      "source": [
        "sdf.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wbOnrlNsz94"
      },
      "source": [
        "#### Task 3: Create a Table View\n",
        "\n",
        "Creating a table view in Spark SQL is required to run SQL queries programmatically on a DataFrame. A view is a temporary table to run SQL queries. A Temporary view provides local scope within the current Spark session. In this example we create a temporary view using the `createTempView()` function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRuCeteQsz94"
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRZOL3zpsz94"
      },
      "source": [
        "## Exercise 3 - Running SQL queries and aggregating data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ9T-6fRsz94"
      },
      "source": [
        "Once we have a table view, we can run queries similar to querying a SQL table. We perform similar operations to the ones in the DataFrames notebook. Note the difference here however is that we use the SQL queries directly.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Show the top 5 rows (use both SQL, Data Frame operations )"
      ],
      "metadata": {
        "id": "SV-HhoSdwLBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Show SepalWidthCm>3 and SepalLengthCm >4.5 and sort by PetalLengthcm (use both SQL, Data Frame operations )"
      ],
      "metadata": {
        "id": "9MAVbQyXwOyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8UUqARVsz95"
      },
      "source": [
        "## Exercise 4 - Create a Pandas UDF to apply a columnar operation\n",
        "\n",
        "Apache Spark has become the de-facto standard in processing big data. To enable data scientists to leverage the value of big data, Spark added a Python API in version 0.7, with support for user-defined functions (UDF). These user-defined functions operate one-row-at-a-time, and thus suffer from high serialization and invocation overhead. As a result, many data pipelines define UDFs in Java and Scala and then invoke them from Python.\n",
        "\n",
        "Pandas UDFs built on top of Apache Arrow bring you the *best of both worlds*â€”the ability to define low-overhead, high-performance UDFs entirely in Python. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb6PmoyDsz95"
      },
      "source": [
        "#### Task 1: Importing libraries and registering a UDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxG0YaXqsz95"
      },
      "outputs": [],
      "source": [
        "# import the Pandas UDF function \n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHBL4Zzjsz95"
      },
      "source": [
        "#### Task 2: Applying the UDF to the tableview\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n",
        "\n",
        "Create new columns that give the standardize value of each column X_STD=( X-mean(X))/SD(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here"
      ],
      "metadata": {
        "id": "x3pmwYR_oYqJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}